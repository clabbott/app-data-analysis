10/28 
Meeting about moving forward with project. 
Before next meeting: put a notebook in the repo, start data collection. :)
11/4
Uploaded initial python Jupyter Notebook project files
Have not started data collection
11/8
Scraped data from datafile, included datafile in repo, wrote up plan to clean data in file
11/16
General Changes:
Imported Numpy
Dataframe originally read from archive file? Changed it to simply the csv file for now.

App Column (Cleaning Database):
Dropped malformed data: "Life Made WI-Fi Touchscreen Photo Frame"
Dropped exact duplicate names.

Reviews Column (Cleaning Database):
Converted to numeric

Size Column (Cleaning Database):
Replaced "Varies with device" value with NaN. 
Converted strings to kb.
12/11
Cleaned the rest of the dataframe except for the genres column 
Might have submitted the Jupyter in a weird way, trying to fix that on the submit for 12/12 
12/12
Formatted everything to make it a bit more organized.
Added introduction. Separated data collection into it's own code block and also gave it a blurb.
Based my writing off the examples in the syllabus. Prose makes up a good amount of our grade so lmk what you think.
We can change it if our project develops as we look more into data.

I've left Data Processing as it is for now. I think splitting up the huge blocks for column may be easier on the eyes.
Also I understand why genre and content rating are split like they are, but I'm worried Teli may expect a "Tidy" dataset for that part.
I can do some of this stuff myself, just wanted your opinion before I messed with what you have.

Graphing was a lot harder than I thought it would be. There are a few directions we could go.
Explained more in the notebook.

12/12
Did a lot of potential exploratory analysis with the aim to compare reviews and installs. 
Separated the dataframes: 
df- dataframe containing no duplicates, apps with multiple genres will only appear once with one of their genres. 
  Use this when comparing the entire dataframe but not the genres.
genre_df- dataframe containing duplicates, apps with multiple genres will appear twice. Do not use this for analysis of the entire dataframe.
ml_df- dataframe containing no duplicates but dummy variables for content rating and genres. Use for machine learning. May not be tidy according to teli's standards. 
Did a simple and poor linear regresion line on "Installs vs Review"
Thoughts: 
- Would eliminating outliers make the data easier to use? 
- What is our target variable that we want to predict? 
More info in notebook
12/13
Created three new chart groups playing around with a more categorical approach to installs.
1st chart is a pie chart that breaks down 10^x bucket percentage which is used to inform the other charts.
2nd and 3rd look at how rating and category change at different install percentiles.
The results are a bit disappointing, but still useful.
Rating distributions only change slightly with minimums and outliers. Category changes with some categories being left of higher percentiles, but still similar across the board.
Overall, I think what little atributes we have aren't decisive enough to make a decision tree.
Ratings being our target variable might be our best bet.
12/15 (early morning)
Did most of data exploration a little behind schedule. Feel free to insert your analysis anywhere in there where you think it would fit, I was just going over all the variables
and how they interact. Today I will finish up the exploration and let you take a shot at ML first because you had some good ideas. More in the doc. 
12/15 (afternoon)
Finished up data exploration except for rating and comparing installs/reviews. You had a really good take on those variables, could you cover those tonight? 
We hit a gold mine with the "Last Updated" variable, theres a clear trend for apps to do better the more recently they were updated. That will be very useful in ML. 
Ratings turned out to be a bust, its the same for everything, just a straight average of 4.1ish. Depending on what you find out with comparing reviews and installs,
I think trying to predict number of installs is our best bet. 
12/16 (Late Night AM)
Started regression in ML and it was kind of a pain to work with as predicted.
Ended up going with Reviews as target instead of installs because
- Doubt decision trees will work as discussed on my 12/13
- Lack of diverse numbers may prove hard to get useful residuals to improve regression models.
If you figure something better out feel free.

I really wanted to find a regression method that would account for our exponential growth in our data.
Couldn't find one, but I feel like we are missing something, also couldn't figure out logarithmic regression so that might be the key idk.
Instead I scaled down the data by taking the log of reviews and installs as it seemed like the easy way to get the job done and we got a decent looking regression line.
I think there are some downsides to this like tightening our data might be misrepresenting (a error of 3 is in reality an error of 1000).

I tried some residual violin graphs to try to expand on your exploratory analysis and prove that we can limit error by separating some of the categories.
Sadly none of the graphs seemed to be skewed in either direction. But, maybe you can make better light of the graphs?
It might be a symptom of tightening our data or maybe we just have a serious outlier problem. 
Also some of your dummy variables have 2 in them, you may be counting categories in the same genre twice? Is this on purpose?

12/17 (Late Night AM)
Wow, what a doozy.
4 different models and I've seen each one preform better then another depending on the training sample.
Good News though: I found a bug in your Multiple Linear Regression Model error calc, you forgot to add the intercept. 
It now preforms better than the normal linear regression model a lot more consistently

Moving forward it's important to keep in mind that there are two possible changes we can make to our base Linear Regression Model:
 - Making it multi variable
 - Using Log Reviews and Log Installs instead

My theories on why some preform better than others:
Multivariable sometimes overfits the training dataset leading to higher error in the test dataset
Most of the time Log preforms worse expect for then the training data set dips heavily into the (< 10^4) range

Some side worries I haven't figured out (Bugs?):
For some reason your poly regression preforms a lot worse than the linear model now, I have no idea why don't think I even touched it
For some reason any model with "Last Updated" as a variable is the same
12/18 (Henry's Last Commit?)
- Updated Imported Library descriptions. 
  Left out scipy.optimize and statsmodel.api bc their unused, idk why they are there and I was afraid of breaking things by removing them. You can try removing them if you want.
- Added a bit more prose to Data Processing. Also split up the code blocks because it was annoying me lol.
- Reorganized Machine Learning a bit.
  - Moved "Determining What The Response Variable Will Be" above our regression model and went in a little more depth and also made it not dependant on information explained below.
  - Original organization of "linear regression" and "Working With Exponentially Increasing Values" didn't really make sense.
    Restructured and added to both sections showing our exponentially increasing values, how we can take the log to make our graph look better but explain why it didn't preform better.
- Added a bit in the conclusion encouraging future exploration. Should help towards Motivation and Other Resources in the rubric.